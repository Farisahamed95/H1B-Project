1a query:
hadoop jar First.jar task1a /niit6/proj /niit5/projop1

1b query:
register /usr/local/hive/lib/hive-exec-1.2.1.jar;
register /usr/local/hive/lib/hive-common-1.2.1.jar;
data1 = LOAD 'hdfs://localhost:54310/user/hive/warehouse/project.db/h1b_final' USING PigStorage('\t') as (s_no:double,case_status:chararray,employer_name:chararray,soc_name:chararray,job_title:chararray,full_time_position:chararray,prevailing_wage:double,year:chararray,worksite:chararray,longitude,latitude);
data2= FILTER data1 by year=='2011';
data3= group data2 by $4;
data4= foreach data3 generate group, COUNT($1);
data5= FILTER data1 by year=='2012';
data6= group data5 by $4;
data7= foreach data6 generate group, COUNT($1);
data8= FILTER data1 by year=='2013';
data9= group data8 by $4;
data10= foreach data9 generate group, COUNT($1);
data11= FILTER data1 by year=='2014';
data12= group data11 by $4;
data13= foreach data12 generate group, COUNT($1);
data14= FILTER data1 by year=='2015';
data15= group data14 by $4;
data16= foreach data15 generate group, COUNT($1);
data17= FILTER data1 by year=='2016';
data18= group data17 by $4;
data19= foreach data18 generate group, COUNT($1);
data20= join data4 by $0,data7 by $0,data10 by $0,data13 by $0,data16 by $0,data19 by $0;
data21= foreach data20 generate $0,$1,$3,$5,$7,$9,$11;
data22= foreach data21  generate $0,(float)($6-$5)*100/$5,(float)($5-$4)*100/$4,(float)($4-$3)*100/$3,(float)($3-$2)*100/$2,(float)($2-$1)*100/$1;
data23= foreach data22 generate $0,($1+$2+$3+$4+$5)/5;
data24= order data23 by $1 desc;
data25 = limit data24 5;
dump data25;

2a query:
select worksite,count(*) as total from h1b_final where job_title like "%DATA ENGINEER%" and year='2011' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where job_title like "%DATA ENGINEER%" and year='2012' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where job_title like "%DATA ENGINEER%" and year='2013' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where job_title like "%DATA ENGINEER%" and year='2014' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where job_title like "%DATA ENGINEER%" and year='2015' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where job_title like "%DATA ENGINEER%" and year='2016' group by worksite order by total desc limit 5;


2b query:
select worksite,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2011' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2012' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2013' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2014' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2015' group by worksite order by total desc limit 5;
select worksite,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2016' group by worksite order by total desc limit 5;

3rd query:
hadoop jar First.jar task3 /niit6/proj /niit5/projop2

4th query:
select employer_name,count(*) as total from h1b_final where year='2011' group by employer_name order by total desc limit 5;
select employer_name,count(*) as total from h1b_final where year='2012' group by employer_name order by total desc limit 5;
select employer_name,count(*) as total from h1b_final where year='2013' group by employer_name order by total desc limit 5;
select employer_name,count(*) as total from h1b_final where year='2014' group by employer_name order by total desc limit 5;
select employer_name,count(*) as total from h1b_final where year='2015' group by employer_name order by total desc limit 5;
select employer_name,count(*) as total from h1b_final where year='2016' group by employer_name order by total desc limit 5;

5a query:
select job_title,count(*) as total from h1b_final where year='2011' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where year='2012' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where year='2013' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where year='2014' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where year='2015' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where year='2016' group by job_title order by total desc limit 10;

5b query:
select job_title,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2011' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2012' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2013' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2014' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2015' group by job_title order by total desc limit 10;
select job_title,count(*) as total from h1b_final where case_status="CERTIFIED" and year='2016' group by job_title order by total desc limit 10;


6th query:
register /usr/local/hive/lib/hive-exec-1.2.1.jar;
register /usr/local/hive/lib/hive-common-1.2.1.jar;
data1 = LOAD 'hdfs://localhost:54310/user/hive/warehouse/project.db/h1b_final' USING PigStorage('\t') as (s_no:double,case_status:chararray,employer_name:chararray,soc_name:chararray,job_title:chararray,full_time_position:chararray,prevailing_wage:double,year:chararray,worksite:chararray,longitude,latitude);
noheader= filter data1 by $0>=1;   --remove header
cleansed= filter noheader by $1 is not null and $1!='NA';
temp= group cleansed  by $7;
total= foreach temp generate group,COUNT(cleansed.$1);
--describe total;dump total;
noheader= filter data1 by $0>=1;
cleansed= filter noheader by $7 is not null and $7!='NA';
temp= group cleansed by ($7,$1);
yearsoccount= foreach temp generate group,group.$0,COUNT($1);
dump yearsoccount;
joined= join yearsoccount by $1,total by $0;
ans= foreach joined generate FLATTEN($0),(float)($2*100)/$4,$2; --percent generation
dump ans;


7th query:
select year, count(*) from h1b_final group by year;

8a query:
register /usr/local/hive/lib/hive-exec-1.2.1.jar;
register /usr/local/hive/lib/hive-common-1.2.1.jar;
data1 = LOAD 'hdfs://localhost:54310/user/hive/warehouse/project.db/h1b_final' USING PigStorage('\t') as (s_no:double,case_status:chararray,employer_name:chararray,soc_name:chararray,job_title:chararray,full_time_position:chararray,prevailing_wage:double,year:chararray,worksite:chararray,longitude,latitude);
data3= FILTER data1 BY case_status=='CERTIFIED' or case_status=='CERTIFIED-WITHDRAWN';
data4= FILTER data3 BY full_time_position=='Y';
data6= foreach data4 generate $1,$4,$5,$6,$7;
describe data6;
data7= group data6 by $4;
--describe data7;
data8 = foreach data7 generate group,ROUND_TO(AVG(data6.prevailing_wage),2) as avg_wage;
dump data8;

8b query:
register /usr/local/hive/lib/hive-exec-1.2.1.jar;
register /usr/local/hive/lib/hive-common-1.2.1.jar;
data1 = LOAD 'hdfs://localhost:54310/user/hive/warehouse/project.db/h1b_final' USING PigStorage('\t') as (s_no:double,case_status:chararray,employer_name:chararray,soc_name:chararray,job_title:chararray,full_time_position:chararray,prevailing_wage:double,year:chararray,worksite:chararray,longitude,latitude);
data3= FILTER data1 BY case_status=='CERTIFIED' or case_status=='CERTIFIED-WITHDRAWN';
data4= FILTER data3 BY full_time_position=='N';
data6= foreach data4 generate $1,$4,$5,$6,$7;
describe data6;
data7= group data6 by $4;
--describe data7;
data8 = foreach data7 generate group,ROUND_TO(AVG(data6.prevailing_wage),2) as avg_wage;
dump data8;

9th query:
register /usr/local/hive/lib/hive-exec-1.2.1.jar;
register /usr/local/hive/lib/hive-common-1.2.1.jar;
data1 = LOAD 'hdfs://localhost:54310/user/hive/warehouse/project.db/h1b_final' USING PigStorage('\t') as (s_no:double,case_status:chararray,employer_name:chararray,soc_name:chararray,job_title:chararray,full_time_position:chararray,prevailing_wage:double,year:chararray,worksite:chararray,longitude,latitude);
data2= group data1 by employer_name;
data3= foreach data2 generate group,COUNT($1);
data4= FILTER data1 by case_status=='CERTIFIED';
data5= group data4 by employer_name;
data6= foreach data5 generate group,COUNT($1);
data7= FILTER data1 by case_status=='CERTIFIED-WITHDRAWN';
data8= group data7 by employer_name;
data9= foreach data8 generate group,COUNT($1);
data10= join data3 by $0, data6 by $0, data9 by $0;
data11= foreach data10 generate $0,$1,$3,$5;
data12= foreach data11 generate $0,(float)($2+$3)*100/($1),$1;
data13= FILTER data12 by $1>70 and $2>1000;
data14= order data13 by $1 DESC;

10th query:
register /usr/local/hive/lib/hive-exec-1.2.1.jar;
register /usr/local/hive/lib/hive-common-1.2.1.jar;
data1 = LOAD 'hdfs://localhost:54310/user/hive/warehouse/project.db/h1b_final' USING PigStorage('\t') as (s_no:double,case_status:chararray,employer_name:chararray,soc_name:chararray,job_title:chararray,full_time_position:chararray,prevailing_wage:double,year:chararray,worksite:chararray,longitude,latitude);
data2= group data1 by job_title;
data3= foreach data2 generate group,COUNT($1);
data4= FILTER data1 by case_status=='CERTIFIED';
data5= group data4 by job_title;
data6= foreach data5 generate group,COUNT($1);
data7= FILTER data1 by case_status=='CERTIFIED-WITHDRAWN';
data8= group data7 by job_title;
data9= foreach data8 generate group,COUNT($1);
data10= join data3 by $0, data6 by $0, data9 by $0;
data11= foreach data10 generate $0,$1,$3,$5;
data12= foreach data11 generate $0,(float)($2+$3)*100/($1),$1;
data13= FILTER data12 by $1>70 and $2>1000;
data14= order data13 by $1 DESC;
store data14 into '/home/hduser/niit/Pig/question10' using PigStorage('\t');

11th query:
sqoop export --connect jdbc:mysql://localhost/question11 --username root --password 'helloworld' --table question11 --update-mode allowinsert  --export-dir /Pig/Question10/p* --input-fields-terminated-by '\t';

hadoop fs -mkdir -p /Pig/Question10
hadoop fs -put /home/hduser/niit1/Pig1/question10/p* /Pig/Question10/
mysql -u root -p'helloworld' -e 'select * from question11.question11';




